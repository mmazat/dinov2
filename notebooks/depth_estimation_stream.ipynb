{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth Estimation <a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/dinov2/blob/main/notebooks/depth_estimation.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "INSTALL = False # Switch this to install dependencies\n",
    "if INSTALL: # Try installing package with extras\n",
    "    REPO_URL = \"https://github.com/facebookresearch/dinov2\"\n",
    "    !{sys.executable} -m pip install -e {REPO_URL}'[extras]' --extra-index-url https://download.pytorch.org/whl/cu117  --extra-index-url https://pypi.nvidia.com\n",
    "else:\n",
    "    REPO_PATH = \"\" # Specify a local path to the repository (or use installed package instead)\n",
    "    sys.path.append(REPO_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dinov2.eval.depth.models import build_depther\n",
    "\n",
    "\n",
    "class CenterPadding(torch.nn.Module):\n",
    "    def __init__(self, multiple):\n",
    "        super().__init__()\n",
    "        self.multiple = multiple\n",
    "\n",
    "    def _get_pad(self, size):\n",
    "        new_size = math.ceil(size / self.multiple) * self.multiple\n",
    "        pad_size = new_size - size\n",
    "        pad_size_left = pad_size // 2\n",
    "        pad_size_right = pad_size - pad_size_left\n",
    "        return pad_size_left, pad_size_right\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, x):\n",
    "        pads = list(itertools.chain.from_iterable(self._get_pad(m) for m in x.shape[:1:-1]))\n",
    "        output = F.pad(x, pads)\n",
    "        return output\n",
    "\n",
    "\n",
    "def create_depther(cfg, backbone_model, backbone_size, head_type):\n",
    "    train_cfg = cfg.get(\"train_cfg\")\n",
    "    test_cfg = cfg.get(\"test_cfg\")\n",
    "    depther = build_depther(cfg.model, train_cfg=train_cfg, test_cfg=test_cfg)\n",
    "\n",
    "    depther.backbone.forward = partial(\n",
    "        backbone_model.get_intermediate_layers,\n",
    "        n=cfg.model.backbone.out_indices,\n",
    "        reshape=True,\n",
    "        return_class_token=cfg.model.backbone.output_cls_token,\n",
    "        norm=cfg.model.backbone.final_norm,\n",
    "    )\n",
    "\n",
    "    if hasattr(backbone_model, \"patch_size\"):\n",
    "        depther.backbone.register_forward_pre_hook(lambda _, x: CenterPadding(backbone_model.patch_size)(x[0]))\n",
    "\n",
    "    return depther"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE_SIZE = \"giant\" # in (\"small\", \"base\", \"large\" or \"giant\")\n",
    "\n",
    "\n",
    "backbone_archs = {\n",
    "    \"small\": \"vits14\",\n",
    "    \"base\": \"vitb14\",\n",
    "    \"large\": \"vitl14\",\n",
    "    \"giant\": \"vitg14\",\n",
    "}\n",
    "backbone_arch = backbone_archs[BACKBONE_SIZE]\n",
    "backbone_name = f\"dinov2_{backbone_arch}\"\n",
    "\n",
    "backbone_model = torch.hub.load(repo_or_dir=\"facebookresearch/dinov2\", model=backbone_name)\n",
    "backbone_model.eval()\n",
    "backbone_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained depth head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "import mmcv\n",
    "from mmcv.runner import load_checkpoint\n",
    "from torchvision import transforms\n",
    "import matplotlib\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def load_config_from_url(url: str) -> str:\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        return f.read().decode()\n",
    "\n",
    "\n",
    "HEAD_DATASET = \"kitti\" # in (\"nyu\", \"kitti\")\n",
    "HEAD_TYPE = \"dpt\" # in (\"linear\", \"linear4\", \"dpt\")\n",
    "\n",
    "\n",
    "DINOV2_BASE_URL = \"https://dl.fbaipublicfiles.com/dinov2\"\n",
    "head_config_url = f\"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_config.py\"\n",
    "head_checkpoint_url = f\"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_head.pth\"\n",
    "\n",
    "cfg_str = load_config_from_url(head_config_url)\n",
    "cfg = mmcv.Config.fromstring(cfg_str, file_format=\".py\")\n",
    "\n",
    "model = create_depther(\n",
    "    cfg,\n",
    "    backbone_model=backbone_model,\n",
    "    backbone_size=BACKBONE_SIZE,\n",
    "    head_type=HEAD_TYPE,\n",
    ")\n",
    "\n",
    "load_checkpoint(model, head_checkpoint_url, map_location=\"cpu\")\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "\n",
    "def make_depth_transform() -> transforms.Compose:\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        lambda x: 255.0 * x[:3], # Discard alpha component and scale by 255\n",
    "        transforms.Normalize(\n",
    "            mean=(123.675, 116.28, 103.53),\n",
    "            std=(58.395, 57.12, 57.375),\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "\n",
    "def render_depth(values, colormap_name=\"magma_r\") -> Image:\n",
    "    min_value, max_value = values.min(), values.max()\n",
    "    normalized_values = (values - min_value) / (max_value - min_value)\n",
    "\n",
    "    colormap = matplotlib.colormaps[colormap_name]\n",
    "    colors = colormap(normalized_values, bytes=True) # ((1)xhxwx4)\n",
    "    colors = colors[:, :, :3] # Discard alpha component\n",
    "    return Image.fromarray(colors)\n",
    "\n",
    "\n",
    "transform = make_depth_transform()\n",
    "\n",
    "def get_depth_img(image:Image,scale_factor=1):\n",
    "    if scale_factor==1:\n",
    "        rescaled_image=image\n",
    "    else:\n",
    "        rescaled_image = image.resize((scale_factor * image.width, scale_factor * image.height))\n",
    "    \n",
    "    transformed_image = transform(rescaled_image)\n",
    "    batch = transformed_image.unsqueeze(0).cuda() # Make a batch of one image\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        result = model.whole_inference(batch, img_meta=None, rescale=True)\n",
    "\n",
    "    depth_image = render_depth(result.squeeze().cpu())\n",
    "    return depth_image\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate depth from camera images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    " # Open a connection to the webcam (0 is the default camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        break\n",
    "    image = Image.fromarray(frame).convert(\"RGB\")\n",
    "\n",
    "  \n",
    "    depth_image=get_depth_img(image)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Webcam Stream', frame)\n",
    "    cv2.imshow('Depth', np.asarray(depth_image))\n",
    "    \n",
    "    # Exit the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Video from image sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "\n",
    "img_folder=Path(\"\")\n",
    "video_file=img_folder/(\"depth_backbone_\"+backbone_archs[BACKBONE_SIZE]+\"_\"+\"head_\"+HEAD_DATASET+\".avi\")\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "fps = 10.0\n",
    "\n",
    "jpg_files = glob.glob(str(img_folder) + \"/*.jpg\")\n",
    "jpg_files.sort()\n",
    "\n",
    "video_initialized=False\n",
    "n=0\n",
    "# Iterate over each image and depth image pair\n",
    "for img in tqdm.tqdm(jpg_files):\n",
    "        image = Image.open(img).convert(\"RGB\")\n",
    "\n",
    "        if not video_initialized:\n",
    "                frame_size = (image.width, image.height*2)  # Adjust to your images' size\n",
    "                video = cv2.VideoWriter(video_file, fourcc, fps, frame_size)\n",
    "                video_initialized=True\n",
    "        \n",
    "        n+=1        \n",
    "        #if n==10:\n",
    "        #        break;\n",
    "                \n",
    "        depth_image = get_depth_img(image)\n",
    "\n",
    "        # Convert the PIL images to numpy arrays\n",
    "        img_array = np.array(image)\n",
    "        depth_array = np.array(depth_image)\n",
    "\n",
    "        # Vertically concatenate the images\n",
    "        concat_image = np.concatenate((depth_array,img_array), axis=0)\n",
    "        \n",
    "        # Clear image from memory\n",
    "        image.close()\n",
    "        depth_image.close()\n",
    "        video.write(concat_image)\n",
    "        \n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
